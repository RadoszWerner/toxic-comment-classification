{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5918886-9105-4912-b2ab-133f393a3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd567f76-f079-4971-bf95-0e7b23ceba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tylko za 1 razem\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee7c733-82c4-47bd-b283-1a47e12a7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre clean:\n",
      "\n",
      " 0         explanation\\nwhy the edits made under my usern...\n",
      "1         d'aww! he matches this background colour i'm s...\n",
      "2         hey man, i'm really not trying to edit war. it...\n",
      "3         \"\\nmore\\ni can't make any real suggestions on ...\n",
      "4         you, sir, are my hero. any chance you remember...\n",
      "                                ...                        \n",
      "159566    \":::::and for the second time of asking, when ...\n",
      "159567    you should be ashamed of yourself \\n\\nthat is ...\n",
      "159568    spitzer \\n\\numm, theres no actual article for ...\n",
      "159569    and it looks like it was actually you who put ...\n",
      "159570    \"\\nand ... i really don't think you understand...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wczytywanie danych\n",
    "train_data = pd.read_csv(\"jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "train_data[\"comment_text\"] = train_data[\"comment_text\"].str.lower()\n",
    "#print(train_data[\"comment_text\"])\n",
    "print(\"pre clean:\\n\\n\", train_data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22b389a-5c2f-4676-8d0d-e1c76d186f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing danych\n",
    "def cleaning(data):\n",
    "    # remove the characters in the first parameter\n",
    "    clean_column = re.sub('<.*?>', ' ', str(data))\n",
    "    # removes non-alphanumeric characters except periods.\n",
    "    clean_column = re.sub('[^a-zA-Z0-9.]+', ' ', clean_column)\n",
    "    # tokenize\n",
    "    tokenized_column = word_tokenize(clean_column)\n",
    "    return tokenized_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb10c85-40bd-4ea3-bd30-ce55d90142af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post clean:\n",
      "\n",
      " 0         [explanation, why, the, edits, made, under, my...\n",
      "1         [d, aww, he, matches, this, background, colour...\n",
      "2         [hey, man, i, m, really, not, trying, to, edit...\n",
      "3         [more, i, can, t, make, any, real, suggestions...\n",
      "4         [you, sir, are, my, hero, ., any, chance, you,...\n",
      "                                ...                        \n",
      "159566    [and, for, the, second, time, of, asking, when...\n",
      "159567    [you, should, be, ashamed, of, yourself, that,...\n",
      "159568    [spitzer, umm, theres, no, actual, article, fo...\n",
      "159569    [and, it, looks, like, it, was, actually, you,...\n",
      "159570    [and, ..., i, really, don, t, think, you, unde...\n",
      "Name: cleaned, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data[\"cleaned\"] = train_data[\"comment_text\"].apply(cleaning)\n",
    "print(\"post clean:\\n\\n\", train_data[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af6ced3-8295-4cbe-9c91-84a664008a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post lemmatize:\n",
      "\n",
      " 0         [explanation, why, the, edits, made, under, my...\n",
      "1         [d, aww, he, match, this, background, colour, ...\n",
      "2         [hey, man, i, m, really, not, trying, to, edit...\n",
      "3         [more, i, can, t, make, any, real, suggestion,...\n",
      "4         [you, sir, are, my, hero, ., any, chance, you,...\n",
      "                                ...                        \n",
      "159566    [and, for, the, second, time, of, asking, when...\n",
      "159567    [you, should, be, ashamed, of, yourself, that,...\n",
      "159568    [spitzer, umm, there, no, actual, article, for...\n",
      "159569    [and, it, look, like, it, wa, actually, you, w...\n",
      "159570    [and, ..., i, really, don, t, think, you, unde...\n",
      "Name: lemmatized, Length: 159571, dtype: object\n",
      "                                             comment_text\n",
      "0       [explanation, why, the, edits, made, under, my...\n",
      "1       [d, aww, he, match, this, background, colour, ...\n",
      "2       [hey, man, i, m, really, not, trying, to, edit...\n",
      "3       [more, i, can, t, make, any, real, suggestion,...\n",
      "4       [you, sir, are, my, hero, ., any, chance, you,...\n",
      "...                                                   ...\n",
      "159566  [and, for, the, second, time, of, asking, when...\n",
      "159567  [you, should, be, ashamed, of, yourself, that,...\n",
      "159568  [spitzer, umm, there, no, actual, article, for...\n",
      "159569  [and, it, look, like, it, wa, actually, you, w...\n",
      "159570  [and, ..., i, really, don, t, think, you, unde...\n",
      "\n",
      "[159571 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lematyzacja slow\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(data):\n",
    "    # input our data in function, take the cleaned column\n",
    "    my_data = data[\"cleaned\"]\n",
    "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_data]\n",
    "    return (lemmatized_list)\n",
    "\n",
    "train_data[\"lemmatized\"] = train_data.apply(lemmatizing, axis=1)\n",
    "print(\"post lemmatize:\\n\\n\", train_data[\"lemmatized\"])\n",
    "\n",
    "train_data[\"comment_text\"] = train_data[\"lemmatized\"]\n",
    "train = train_data[[\"comment_text\"]]\n",
    "train_labels = train_data[[\"toxic\"]]\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3790c031-079d-4709-a70a-0f82a39bad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2. Używanie train_test_split w celu podziału na podzbior testowy i treningowy\n",
    "comment_train, comment_test, labels_train, labels_test = train_test_split(train, train_labels, test_size=0.2,\n",
    "                                                                          random_state=42)\n",
    "# Transpozycja itp w celu zmiany macierzy do odpowiedniej formy\n",
    "labels_train = np.transpose(labels_train)\n",
    "labels_train = np.ravel(labels_train)\n",
    "labels_test = np.transpose(labels_test)\n",
    "labels_test = np.ravel(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3725d50-13d2-456f-ae08-ced7ada497ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 59372)\t3\n",
      "  (0, 132633)\t3\n",
      "  (0, 121844)\t1\n",
      "  (0, 25741)\t1\n",
      "  (0, 67740)\t1\n",
      "  (0, 136244)\t2\n",
      "  (0, 70748)\t1\n",
      "  (0, 62154)\t1\n",
      "  (0, 63119)\t1\n",
      "  (0, 134938)\t1\n",
      "  (0, 62865)\t1\n",
      "  (0, 6641)\t1\n",
      "  (0, 6739)\t1\n",
      "  (0, 6817)\t1\n",
      "  (0, 4984)\t1\n",
      "  (1, 67740)\t1\n",
      "  (1, 70748)\t1\n",
      "  (1, 134938)\t5\n",
      "  (1, 85178)\t1\n",
      "  (1, 2725)\t1\n",
      "  (1, 141033)\t1\n",
      "  (1, 71074)\t1\n",
      "  (1, 147886)\t3\n",
      "  (1, 19889)\t3\n",
      "  (1, 45223)\t1\n",
      "  :\t:\n",
      "  (127654, 103102)\t1\n",
      "  (127654, 127929)\t1\n",
      "  (127654, 32135)\t1\n",
      "  (127654, 15520)\t1\n",
      "  (127654, 80929)\t1\n",
      "  (127654, 55949)\t1\n",
      "  (127654, 95742)\t1\n",
      "  (127654, 123374)\t1\n",
      "  (127654, 87166)\t1\n",
      "  (127655, 67740)\t1\n",
      "  (127655, 70748)\t1\n",
      "  (127655, 149637)\t1\n",
      "  (127655, 133105)\t1\n",
      "  (127655, 12916)\t1\n",
      "  (127655, 15071)\t1\n",
      "  (127655, 148153)\t1\n",
      "  (127655, 86513)\t1\n",
      "  (127655, 97959)\t1\n",
      "  (127655, 73758)\t1\n",
      "  (127655, 35328)\t1\n",
      "  (127655, 35290)\t1\n",
      "  (127655, 129319)\t1\n",
      "  (127655, 133878)\t1\n",
      "  (127655, 42997)\t1\n",
      "  (127655, 39863)\t1\n"
     ]
    }
   ],
   "source": [
    "# 3. CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "comment_train_counts = count_vect.fit_transform(comment_train.comment_text.astype(str))\n",
    "print(comment_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9907359-c770-4f44-bf48-52d4d17ac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. TfidfTransformer\n",
    "#Use tf-idf instead\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(comment_train_counts)\n",
    "comment_train_tf = tf_transformer.transform(comment_train_counts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "comment_train_tfidf = tfidf_transformer.fit_transform(comment_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72351486-baed-4a3d-ae8c-5bbf1f56d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "# tworzenie modelu\n",
    "clf = MultinomialNB().fit(comment_train_tfidf, labels_train)\n",
    "\n",
    "# pakowanie slow w paczki\n",
    "comment_test_new_counts = count_vect.transform(comment_test.comment_text.astype(str))\n",
    "comment_test_new_tfidf = tfidf_transformer.transform(comment_test_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82aecdd7-e736-45a7-8f22-758fc4b17854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">71,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │        \u001b[38;5;34m71,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m5,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,636,701</span> (10.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,636,701\u001b[0m (10.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,636,701</span> (10.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,636,701\u001b[0m (10.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.9127 - loss: 0.2942\n",
      "Epoch 1: val_loss improved from inf to 0.10430, saving model to save_best_model.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 304ms/step - accuracy: 0.9128 - loss: 0.2938 - val_accuracy: 0.9617 - val_loss: 0.1043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22d952fde50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6 Trenowanie modelu LSTM\n",
    "num_words = 20000\n",
    "max_len = 150\n",
    "emb_size = 128\n",
    "\n",
    "# vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) \n",
    "# or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf\n",
    "tok = Tokenizer(num_words = num_words, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tok.fit_on_texts(list(comment_train.comment_text.astype(str)))\n",
    "\n",
    "comment_train2 = tok.texts_to_sequences(comment_train.comment_text.astype(str))\n",
    "comment_test2 = tok.texts_to_sequences(comment_test.comment_text.astype(str))\n",
    "\n",
    "comment_train2 = sequence.pad_sequences(comment_train2, maxlen = max_len)\n",
    "comment_test2 = sequence.pad_sequences(comment_test2, maxlen = max_len)\n",
    "\n",
    "\n",
    "inp = Input(shape = (max_len, ))\n",
    "layer = Embedding(num_words, emb_size)(inp)\n",
    "layer = Bidirectional(LSTM(50, return_sequences = True, recurrent_dropout = 0.15))(layer)\n",
    "layer = GlobalMaxPool1D()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(50, activation = 'relu')(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(1, activation = 'sigmoid')(layer)\n",
    "model = Model(inputs = inp, outputs = layer)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "\n",
    "file_path = 'save_best_model.keras'\n",
    "checkpoint = ModelCheckpoint(file_path, monitor = 'val_loss', verbose = 1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "model.fit(comment_train2, labels_train, batch_size = 512, epochs = 1, validation_split = 0.2, validation_data = (comment_test2, labels_test), callbacks = [checkpoint, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71043a39-43f8-4aaf-abf3-8767b6eab00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m prediction_nb \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(comment_test_new_tfidf)\n\u001b[0;32m      3\u001b[0m prediction_lstm \u001b[38;5;241m=\u001b[39m (model\u001b[38;5;241m.\u001b[39mpredict(comment_test2)\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0\u001b[39m \n\u001b[1;32m----> 5\u001b[0m cm_nb \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241m.\u001b[39mconfusion_matrix(labels_test, prediction_nb)\n\u001b[0;32m      6\u001b[0m cmd_nb \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(cm_nb, display_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_toxic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxic\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m cmd_nb\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    " #6 Prediction:\n",
    "prediction_nb = clf.predict(comment_test_new_tfidf)\n",
    "prediction_lstm = (model.predict(comment_test2).ravel()>0.5)+0 \n",
    "\n",
    "cm_nb = metrics.confusion_matrix(labels_test, prediction_nb)\n",
    "cmd_nb = ConfusionMatrixDisplay(cm_nb, display_labels=[\"non_toxic\", \"toxic\"])\n",
    "cmd_nb.plot()\n",
    "plt.savefig(\"CM_NB.png\")\n",
    "\n",
    "cm_lstm = confusion_matrix(labels_test, prediction_lstm)\n",
    "cmd_lstm = ConfusionMatrixDisplay(cm_lstm, display_labels=[\"non_toxic\", \"toxic\"])\n",
    "cmd_lstm.plot()\n",
    "plt.savefig(\"CM_LSTM.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    print(\"NB Accuracy:\", file=f)\n",
    "    print(np.mean(prediction_nb == labels_test), file=f)\n",
    "    print(\"\\n\", file=f)\n",
    "    print(\"NB Precision, Recall, and F1 Score:\", file=f)\n",
    "    print(metrics.classification_report(labels_test, prediction_nb), file=f)\n",
    "    print(\"NB Confusion Matrix:\", file=f)\n",
    "    print(cm_nb, file=f)\n",
    "    print(\"\\n\", file=f)\n",
    "\n",
    "    print(\"LSTM Accuracy:\", file=f)\n",
    "    print(np.mean(prediction_lstm == labels_test), file=f)\n",
    "    print(\"\\n\", file=f)\n",
    "    print(\"LSTM Precision, Recall, and F1 Score:\", file=f)\n",
    "    print(metrics.classification_report(labels_test, prediction_lstm), file=f)\n",
    "    print(\"LSTM Confusion Matrix:\", file=f)\n",
    "    print(cm_lstm, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf6130-a56f-4b38-9754-96e7f971ecfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1292917-0108-4b62-8306-df6071863d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
