{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5918886-9105-4912-b2ab-133f393a3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd567f76-f079-4971-bf95-0e7b23ceba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tylko za 1 razem\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee7c733-82c4-47bd-b283-1a47e12a7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre clean:\n",
      "\n",
      " 0         explanation\\nwhy the edits made under my usern...\n",
      "1         d'aww! he matches this background colour i'm s...\n",
      "2         hey man, i'm really not trying to edit war. it...\n",
      "3         \"\\nmore\\ni can't make any real suggestions on ...\n",
      "4         you, sir, are my hero. any chance you remember...\n",
      "                                ...                        \n",
      "159566    \":::::and for the second time of asking, when ...\n",
      "159567    you should be ashamed of yourself \\n\\nthat is ...\n",
      "159568    spitzer \\n\\numm, theres no actual article for ...\n",
      "159569    and it looks like it was actually you who put ...\n",
      "159570    \"\\nand ... i really don't think you understand...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wczytywanie danych\n",
    "train_data = pd.read_csv(\"jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "train_data[\"comment_text\"] = train_data[\"comment_text\"].str.lower()\n",
    "#print(train_data[\"comment_text\"])\n",
    "print(\"pre clean:\\n\\n\", train_data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22b389a-5c2f-4676-8d0d-e1c76d186f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing danych\n",
    "def cleaning(data):\n",
    "    # remove the characters in the first parameter\n",
    "    clean_column = re.sub('<.*?>', ' ', str(data))\n",
    "    # removes non-alphanumeric characters except periods.\n",
    "    clean_column = re.sub('[^a-zA-Z0-9.]+', ' ', clean_column)\n",
    "    # tokenize\n",
    "    tokenized_column = word_tokenize(clean_column)\n",
    "    return tokenized_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb10c85-40bd-4ea3-bd30-ce55d90142af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post clean:\n",
      "\n",
      " 0         [explanation, why, the, edits, made, under, my...\n",
      "1         [d, aww, he, matches, this, background, colour...\n",
      "2         [hey, man, i, m, really, not, trying, to, edit...\n",
      "3         [more, i, can, t, make, any, real, suggestions...\n",
      "4         [you, sir, are, my, hero, ., any, chance, you,...\n",
      "                                ...                        \n",
      "159566    [and, for, the, second, time, of, asking, when...\n",
      "159567    [you, should, be, ashamed, of, yourself, that,...\n",
      "159568    [spitzer, umm, theres, no, actual, article, fo...\n",
      "159569    [and, it, looks, like, it, was, actually, you,...\n",
      "159570    [and, ..., i, really, don, t, think, you, unde...\n",
      "Name: cleaned, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data[\"cleaned\"] = train_data[\"comment_text\"].apply(cleaning)\n",
    "print(\"post clean:\\n\\n\", train_data[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6ced3-8295-4cbe-9c91-84a664008a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematyzacja slow\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(data):\n",
    "    # input our data in function, take the cleaned column\n",
    "    my_data = data[\"cleaned\"]\n",
    "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_data]\n",
    "    return (lemmatized_list)\n",
    "\n",
    "train_data[\"lemmatized\"] = train_data.apply(lemmatizing, axis=1)\n",
    "print(\"post lemmatize:\\n\\n\", train_data[\"lemmatized\"])\n",
    "\n",
    "train_data[\"comment_text\"] = train_data[\"lemmatized\"]\n",
    "train = train_data[[\"comment_text\"]]\n",
    "train_labels = train_data[[\"toxic\"]]\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790c031-079d-4709-a70a-0f82a39bad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2. Use train_test_split to split into train/test\n",
    "comment_train, comment_test, labels_train, labels_test = train_test_split(train, train_labels, test_size=0.2,\n",
    "                                                                          random_state=42)\n",
    "# Transpose and flatten so it fits the correct dimensions\n",
    "labels_train = np.transpose(labels_train)\n",
    "labels_train = np.ravel(labels_train)\n",
    "labels_test = np.transpose(labels_test)\n",
    "labels_test = np.ravel(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3725d50-13d2-456f-ae08-ced7ada497ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "comment_train_counts = count_vect.fit_transform(comment_train.comment_text.astype(str))\n",
    "print(comment_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9907359-c770-4f44-bf48-52d4d17ac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. TfidfTransformer\n",
    "#Use tf-idf instead\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(comment_train_counts)\n",
    "comment_train_tf = tf_transformer.transform(comment_train_counts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "comment_train_tfidf = tfidf_transformer.fit_transform(comment_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72351486-baed-4a3d-ae8c-5bbf1f56d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "# tworzenie modelu\n",
    "clf = MultinomialNB().fit(comment_train_tfidf, labels_train)\n",
    "\n",
    "# pakowanie slow w paczki\n",
    "comment_test_new_counts = count_vect.transform(comment_test.comment_text.astype(str))\n",
    "comment_test_new_tfidf = tfidf_transformer.transform(comment_test_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aecdd7-e736-45a7-8f22-758fc4b17854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Trenowanie modelu LSTM\n",
    "num_words = 20000\n",
    "max_len = 150\n",
    "emb_size = 128\n",
    "\n",
    "# vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) \n",
    "# or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf\n",
    "tok = Tokenizer(num_words = num_words, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tok.fit_on_texts(list(comment_train.comment_text.astype(str)))\n",
    "\n",
    "comment_train2 = tok.texts_to_sequences(comment_train.comment_text.astype(str))\n",
    "comment_test2 = tok.texts_to_sequences(comment_test.comment_text.astype(str))\n",
    "\n",
    "comment_train2 = sequence.pad_sequences(comment_train2, maxlen = max_len)\n",
    "comment_test2 = sequence.pad_sequences(comment_test2, maxlen = max_len)\n",
    "\n",
    "\n",
    "inp = Input(shape = (max_len, ))\n",
    "layer = Embedding(num_words, emb_size)(inp)\n",
    "layer = Bidirectional(LSTM(50, return_sequences = True, recurrent_dropout = 0.15))(layer)\n",
    "layer = GlobalMaxPool1D()(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(50, activation = 'relu')(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(1, activation = 'sigmoid')(layer)\n",
    "model = Model(inputs = inp, outputs = layer)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "\n",
    "file_path = 'save_best_model.keras'\n",
    "checkpoint = ModelCheckpoint(file_path, monitor = 'val_loss', verbose = 1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "model.fit(comment_train2, labels_train, batch_size = 512, epochs = 1, validation_split = 0.2, validation_data = (comment_test2, labels_test), callbacks = [checkpoint, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71043a39-43f8-4aaf-abf3-8767b6eab00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf6130-a56f-4b38-9754-96e7f971ecfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1292917-0108-4b62-8306-df6071863d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
